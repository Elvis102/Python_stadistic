
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>18. Linear regression &#8212; Learning Statistics with Python</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="19. Factorial ANOVA" href="05.05-anova2.html" />
    <link rel="prev" title="16. Comparing several means" href="05.03-anova.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Learning Statistics with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="landingpage.html">
   Learning Statistics with Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part I. Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01.01-intro.html">
   1. Why do we learn statistics?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01.02-studydesign.html">
   2. A brief introduction to research design
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part II. An Introduction to Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02.01-getting_started_with_python.html">
   3. Getting Started with Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02.02-more_python_concepts.html">
   5. More Python Concepts
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part III. Working With Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="03.01-descriptives.html">
   6. Descriptive statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03.02-drawing_graphs.html">
   7. Drawing Graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03.03-pragmatic_matters.html">
   8. Data Wrangling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03.04-basic_programming.html">
   9. Basic Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part IV. Statistical Theory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04.01-intro-to-probability.html">
   10. Statistical theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04.02-probability.html">
   11. Introduction to Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04.03-estimation.html">
   12. Estimating unknown quantities from a sample
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04.04-hypothesis-testing.html">
   13. Hypothesis Testing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part V. Statistical Tools
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="05.01-chisquare.html">
   14. Categorical data analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05.02-ttest.html">
   15. Comparing Two Means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05.03-anova.html">
   16. Comparing several means
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   18. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05.05-anova2.html">
   19. Factorial ANOVA
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part VI. Endings, Alternatives and Prospects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="06.01-bayes.html">
   20. Bayesian Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06.02-epilogue.html">
   21. Epilogue
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   22. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/05.04-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ethanweed/pythonbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ethanweed/pythonbook/issues/new?title=Issue%20on%20page%20%2F05.04-regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ethanweed/pythonbook/https://github.com/ethanweed/pythonbook/tree/main/Chapters?urlpath=tree/05.04-regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-a-linear-regression-model">
   18.1. Estimating a linear regression model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-with-python">
   18.2. Linear Regression with Python
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#warning">
     18.2.1. Warning!!!
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-the-estimated-model">
     18.2.2. Interpreting the estimated model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression">
   18.3. Multiple linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression-in-python">
   18.4. Multiple Linear Regression in Python
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formula-for-the-general-case">
     18.4.1. Formula for the general case
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifying-the-fit-of-the-regression-model">
   18.5. Quantifying the fit of the regression model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-r-2-value">
     18.5.1. The
     <span class="math notranslate nohighlight">
      \(R^2\)
     </span>
     value
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-relationship-between-regression-and-correlation">
     18.5.2. The relationship between regression and correlation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-adjusted-r-2-value">
     18.5.3. The adjusted
     <span class="math notranslate nohighlight">
      \(R^2\)
     </span>
     value
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hypothesis-tests-for-regression-models">
   18.6. Hypothesis tests for regression models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-model-as-a-whole">
     18.6.1. Testing the model as a whole
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tests-for-individual-coefficients">
     18.6.2. Tests for individual coefficients
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-a-linear-regression-model">
   18.1. Estimating a linear regression model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression-with-python">
   18.2. Linear Regression with Python
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#warning">
     18.2.1. Warning!!!
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-the-estimated-model">
     18.2.2. Interpreting the estimated model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression">
   18.3. Multiple linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression-in-python">
   18.4. Multiple Linear Regression in Python
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formula-for-the-general-case">
     18.4.1. Formula for the general case
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifying-the-fit-of-the-regression-model">
   18.5. Quantifying the fit of the regression model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-r-2-value">
     18.5.1. The
     <span class="math notranslate nohighlight">
      \(R^2\)
     </span>
     value
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-relationship-between-regression-and-correlation">
     18.5.2. The relationship between regression and correlation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-adjusted-r-2-value">
     18.5.3. The adjusted
     <span class="math notranslate nohighlight">
      \(R^2\)
     </span>
     value
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hypothesis-tests-for-regression-models">
   18.6. Hypothesis tests for regression models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-model-as-a-whole">
     18.6.1. Testing the model as a whole
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tests-for-individual-coefficients">
     18.6.2. Tests for individual coefficients
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-regression">
<span id="regression"></span><h1><span class="section-number">18. </span>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<p>The goal in this chapter is to introduce <strong><em>linear regression</em></strong>. Stripped to its bare essentials, linear regression models are basically a slightly fancier version of the <span class="xref myst">Pearson correlation</span>, though as we’ll see, regression models are much more powerful tools.</p>
<p>Since the basic ideas in regression are closely tied to correlation, we’ll return to the <code class="docutils literal notranslate"><span class="pre">parenthood.csv</span></code> file that we were using to illustrate how correlations work. Recall that, in this data set, we were trying to find out why Dan is so very grumpy all the time, and our working hypothesis was that I’m not getting enough sleep.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">file</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/ethanweed/pythonbook/main/Data/parenthood.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dan_sleep</th>
      <th>baby_sleep</th>
      <th>dan_grump</th>
      <th>day</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.59</td>
      <td>10.18</td>
      <td>56</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.91</td>
      <td>11.66</td>
      <td>60</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.14</td>
      <td>7.92</td>
      <td>82</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.71</td>
      <td>9.61</td>
      <td>55</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6.68</td>
      <td>9.75</td>
      <td>67</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We drew some scatterplots to help us examine the relationship between the amount of sleep I get, and my grumpiness the following day.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span>
                <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> 
                <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Grumpiness and sleep&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;My grumpiness (0-100)&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;My sleep (hours)&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;sleepycorrelation_fig&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.04-regression_5_1.png" src="_images/05.04-regression_5_1.png" />
</div>
</div>
<div class="figure align-default" id="fig-sleepycorrelation" style="width: 600px">
<p class="caption"><span class="caption-number">Fig. 18.1 </span><span class="caption-text">Scatterplot showing grumpiness as a function of hours slept.</span><a class="headerlink" href="#fig-sleepycorrelation" title="Permalink to this image">¶</a></p>
</div>
<p>The actual scatterplot that we draw is the one shown in <a class="reference internal" href="#fig-sleepycorrelation"><span class="std std-numref">Fig. 18.1</span></a>, and as we saw previously this corresponds to a correlation of <span class="math notranslate nohighlight">\(r=-.90\)</span>, but what we find ourselves secretly imagining is something that looks closer to the left panel in <a class="reference internal" href="#fig-sleep-regressions-1"><span class="std std-numref">Fig. 18.2</span></a>. That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a <strong><em>regression line</em></strong>. Notice that – since we’re not idiots – the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in the right panel in <a class="reference internal" href="#fig-sleep-regressions-1"><span class="std std-numref">Fig. 18.2</span></a>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1"># find the regression coefficients to allow manually plotting the line</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;dan_grump ~ dan_sleep&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">Intercept</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dan_sleep</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>


<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;The best-fitting regression line&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">slope</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">intercept</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Not the best-fitting regression line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">80</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="c1">#glue(&quot;sleep_regressions_1-fig&quot;, fig, display=False)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.04-regression_8_0.png" src="_images/05.04-regression_8_0.png" />
</div>
</div>
<div class="figure align-default" id="fig-sleep-regressions-1" style="width: 600px">
<p class="caption"><span class="caption-number">Fig. 18.2 </span><span class="caption-text">The panel to the left shows the sleep-grumpiness scatterplot from <a class="reference internal" href="#fig-sleepycorrelation"><span class="std std-numref">Fig. 18.1</span></a> with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, the panel to the right shows the same data, but with a very poor choice of regression line drawn over the top.</span><a class="headerlink" href="#fig-sleep-regressions-1" title="Permalink to this image">¶</a></p>
</div>
<p>This is not highly surprising: the line that I’ve drawn in panel to the right doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this:</p>
<div class="math notranslate nohighlight">
\[
y = mx + c
\]</div>
<p>Or, at least, that’s what it was when I went to high school all those years ago. The two <em>variables</em> are <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and we have two <em>coefficients</em>, <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(c\)</span>. The coefficient <span class="math notranslate nohighlight">\(m\)</span> represents the <em>slope</em> of the line, and the coefficient <span class="math notranslate nohighlight">\(c\)</span> represents the <em><span class="math notranslate nohighlight">\(y\)</span>-intercept</em> of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of <span class="math notranslate nohighlight">\(y\)</span> that you get when <span class="math notranslate nohighlight">\(x=0\)</span>”. Similarly, a slope of <span class="math notranslate nohighlight">\(m\)</span> means that if you increase the <span class="math notranslate nohighlight">\(x\)</span>-value by 1 unit, then the <span class="math notranslate nohighlight">\(y\)</span>-value goes up by <span class="math notranslate nohighlight">\(m\)</span> units; a negative slope means that the <span class="math notranslate nohighlight">\(y\)</span>-value would go down rather than up. Ah yes, it’s all coming back to me now.</p>
<p>Now that we’ve remembered that, it should come as no surprise to discover that we use the exact same formula to describe a regression line. If <span class="math notranslate nohighlight">\(Y\)</span> is the outcome variable (the DV) and <span class="math notranslate nohighlight">\(X\)</span> is the predictor variable (the IV), then the formula that describes our regression is written like this:</p>
<div class="math notranslate nohighlight">
\[
\hat{Y_i} = b_1 X_i + b_0
\]</div>
<p>Hm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(Y_i\)</span> rather than just plain old <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. This is because we want to remember that we’re dealing with actual data. In this equation, <span class="math notranslate nohighlight">\(X_i\)</span> is the value of predictor variable for the <span class="math notranslate nohighlight">\(i\)</span>th observation (i.e., the number of hours of sleep that I got on day <span class="math notranslate nohighlight">\(i\)</span> of my little study), and <span class="math notranslate nohighlight">\(Y_i\)</span> is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all <span class="math notranslate nohighlight">\(i\)</span>). Secondly, notice that I wrote <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> and not <span class="math notranslate nohighlight">\(Y_i\)</span>. This is because we want to make the distinction between the <em>actual data</em> <span class="math notranslate nohighlight">\(Y_i\)</span>, and the <em>estimate</em> <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(c\)</span> to <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(b_0\)</span>. That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose <span class="math notranslate nohighlight">\(b\)</span>, but that’s what they did. In any case <span class="math notranslate nohighlight">\(b_0\)</span> always refers to the intercept term, and <span class="math notranslate nohighlight">\(b_1\)</span> refers to the slope.</p>
<p>Excellent, excellent. Next, I can’t help but notice that – regardless of whether we’re talking about the good regression line or the bad one – the data don’t fall perfectly on the line. Or, to say it another way, the data <span class="math notranslate nohighlight">\(Y_i\)</span> are not identical to the predictions of the regression model <span class="math notranslate nohighlight">\(\hat{Y_i}\)</span>. Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a <em>residual</em>, and we’ll refer to it as <span class="math notranslate nohighlight">\(\epsilon_i\)</span>.<a class="footnote-reference brackets" href="#noteepsilon" id="id1">1</a> Written using mathematics, the residuals are defined as:</p>
<div class="math notranslate nohighlight">
\[
\epsilon_i = Y_i - \hat{Y}_i
\]</div>
<p>which in turn means that we can write down the complete linear regression model as:</p>
<div class="math notranslate nohighlight">
\[
Y_i = b_1 X_i + b_0 + \epsilon_i
\]</div>
<div class="section" id="estimating-a-linear-regression-model">
<span id="regressionestimation"></span><h2><span class="section-number">18.1. </span>Estimating a linear regression model<a class="headerlink" href="#estimating-a-linear-regression-model" title="Permalink to this headline">¶</a></h2>
<p>Okay, now let’s redraw our pictures, but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in the left panel of <a class="reference internal" href="#fig-sleep-regressions-2"><span class="std std-numref">Fig. 18.3</span></a>, but when the regression line is a bad one, the residuals are a lot larger, as you can see from looking at the right panel of <a class="reference internal" href="#fig-sleep-regressions-2"><span class="std std-numref">Fig. 18.3</span></a>. Hm. Maybe what we “want” in a regression model is <em>small</em> residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that …</p>
<blockquote>
<div><p>The estimated regression coefficients, <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_1\)</span> are those that minimise the sum of the squared residuals, which we could either write as <span class="math notranslate nohighlight">\(\sum_i (Y_i - \hat{Y}_i)^2\)</span> or as <span class="math notranslate nohighlight">\(\sum_i {\epsilon_i}^2\)</span>.</p>
</div></blockquote>
<p>Yes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are <em>estimates</em> (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_1\)</span> rather than <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>. Finally, I should also note that – since there’s actually more than one way to estimate a regression model – the more technical name for this estimation process is <strong><em>ordinary least squares (OLS) regression</em></strong>.</p>
<p>At this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_1\)</span>. The natural question to ask next is,  if our optimal regression coefficients are those that minimise the sum squared residuals, how do we <em>find</em> these wonderful numbers? The actual answer to this question is complicated, and it doesn’t help you understand the logic of regression.<a class="footnote-reference brackets" href="#notekungfu" id="id2">2</a> As a result, this time I’m going to let you off the hook. Instead of showing you how to do it the long and tedious way first, and then “revealing” the wonderful shortcut that Python provides you with, let’s cut straight to the chase… and use Python to do all the heavy lifting.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span><span class="o">,</span> <span class="nn">scipy</span><span class="o">,</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span>

<span class="n">xData</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span>
<span class="n">yData</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>

<span class="c1"># (the solution to this figure stolen shamelessly from this stack-overflow answer by James Phillips:</span>
<span class="c1"># https://stackoverflow.com/questions/53779773/python-linear-regression-best-fit-line-with-residuals)</span>

<span class="c1"># fit linear regression model and save parameters</span>
<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">initialParameters</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="n">fittedParameters</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">xData</span><span class="p">,</span> <span class="n">yData</span><span class="p">,</span> <span class="n">initialParameters</span><span class="p">)</span>

<span class="n">modelPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xData</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span> 

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">xData</span><span class="p">,</span>
                     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">yData</span><span class="p">})</span>

<span class="c1"># plot data points</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;The best-fitting regression line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>

<span class="c1"># add regression line</span>
<span class="n">xModel</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">xData</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">xData</span><span class="p">))</span>
<span class="n">yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">fittedParameters</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xModel</span><span class="p">,</span> <span class="n">yModel</span><span class="p">)</span>

<span class="c1"># add drop lines</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xData</span><span class="p">)):</span>
    <span class="n">lineXdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># same X</span>
    <span class="n">lineYdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">yData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">modelPredictions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># different Y</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lineXdata</span><span class="p">,</span> <span class="n">lineYdata</span><span class="p">)</span>

    
<span class="c1">#####</span>

<span class="c1"># create poor-fitting model</span>
<span class="n">badParameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
<span class="n">badPredictions</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">xData</span><span class="p">,</span> <span class="o">*</span><span class="n">badParameters</span><span class="p">)</span> 

<span class="n">bad_xModel</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">xData</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">xData</span><span class="p">))</span>
<span class="n">bad_yModel</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">bad_xModel</span><span class="p">,</span> <span class="o">*</span><span class="n">badParameters</span><span class="p">)</span>

<span class="c1"># plot data with poor-fitting model</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Not the best-fitting regression line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;My sleep (hours)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;My grumpiness (0-10)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bad_xModel</span><span class="p">,</span> <span class="n">bad_yModel</span><span class="p">)</span>  

<span class="c1"># add drop lines</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xData</span><span class="p">)):</span>
    <span class="n">lineXdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xData</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
    <span class="n">lineYdata</span> <span class="o">=</span> <span class="p">(</span><span class="n">yData</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">badPredictions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lineXdata</span><span class="p">,</span> <span class="n">lineYdata</span><span class="p">)</span>
  
    
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.04-regression_12_0.png" src="_images/05.04-regression_12_0.png" />
</div>
</div>
<div class="figure align-default" id="fig-sleep-regressions-2" style="width: 600px">
<p class="caption"><span class="caption-number">Fig. 18.3 </span><span class="caption-text">A depiction of the residuals associated with the best fitting regression line (left panel), and the residuals associated with a poor regression line (right panel). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data.</span><a class="headerlink" href="#fig-sleep-regressions-2" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="linear-regression-with-python">
<span id="pingouinregression"></span><h2><span class="section-number">18.2. </span>Linear Regression with Python<a class="headerlink" href="#linear-regression-with-python" title="Permalink to this headline">¶</a></h2>
<p>As always, there are several different ways we could go about calculating a linear regression in Python, but we’ll stick with <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>, which for my money is one of the simplest and easiest packages to use. The <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> command for linear regression is, well, <code class="docutils literal notranslate"><span class="pre">linear_regression</span></code>, so that couldn’t be much more straightforward. After that, we just need to tell <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code> which variable we want to use as a predictor variable (independent variable), and which one we want to use as the outcome variable (dependent variable). <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> wants the predictor variable first, so, since we want to model my grumpiness as a function of my sleep, we write:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display results, rounded to two decimal places.</span>
<span class="n">lm</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.96</td>
      <td>3.02</td>
      <td>41.76</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.97</td>
      <td>131.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.94</td>
      <td>0.43</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-9.79</td>
      <td>-8.09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As is its way, <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> gives us a nice simple table, with a lot of information. Most importantly for now, we can see that <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> has caclulated the intercept <span class="math notranslate nohighlight">\(\hat{b}_0 = 125.96\)</span> and the slope <span class="math notranslate nohighlight">\(\hat{b}_1 = -8.94\)</span>. In other words, the best-fitting regression line that I plotted in <a class="reference internal" href="#fig-sleep-regressions-1"><span class="std std-numref">Fig. 18.2</span></a> has this formula:</p>
<div class="math notranslate nohighlight">
\[
\hat{Y}_i = -8.94 \ X_i + 125.96
\]</div>
<div class="section" id="warning">
<h3><span class="section-number">18.2.1. </span>Warning!!!<a class="headerlink" href="#warning" title="Permalink to this headline">¶</a></h3>
<p>Remember, it’s critical that you put the variables in the right order. If you reverse the predictor and outcome variables, <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code> will happily calculate a result for you, but it will not be the one you are looking for. If instead, we had written <code class="docutils literal notranslate"><span class="pre">pg.linear_regression(df['dan_grump'],</span> <span class="pre">df['dan_sleep'])</span></code>, we would get the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">])</span>
<span class="n">lm</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>12.78</td>
      <td>0.28</td>
      <td>45.27</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>12.22</td>
      <td>13.34</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_grump</td>
      <td>-0.09</td>
      <td>0.00</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.10</td>
      <td>-0.08</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The output looks valid enough on the face of it, and it is even statistically significant. But in this model, we just predicted my son’s sleepiness as a function of my grumpiness, which is madness! Reversing the direction of causality would make a great scifi movie<a class="footnote-reference brackets" href="#notenolan" id="id3">3</a>, but it’s no good in statistics. So remember, predictor first, outcome second<a class="footnote-reference brackets" href="#noteformula" id="id4">4</a></p>
</div>
<div class="section" id="interpreting-the-estimated-model">
<h3><span class="section-number">18.2.2. </span>Interpreting the estimated model<a class="headerlink" href="#interpreting-the-estimated-model" title="Permalink to this headline">¶</a></h3>
<p>The most important thing to be able to understand is how to interpret these coefficients. Let’s start with <span class="math notranslate nohighlight">\(\hat{b}_1\)</span>, the slope. If we remember the definition of the slope, a regression coefficient of <span class="math notranslate nohighlight">\(\hat{b}_1 = -8.94\)</span> means that if I increase <span class="math notranslate nohighlight">\(X_i\)</span> by 1, then I’m decreasing <span class="math notranslate nohighlight">\(Y_i\)</span> by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since <span class="math notranslate nohighlight">\(\hat{b}_0\)</span> corresponds to “the expected value of <span class="math notranslate nohighlight">\(Y_i\)</span> when <span class="math notranslate nohighlight">\(X_i\)</span> equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (<span class="math notranslate nohighlight">\(X_i =0\)</span>) then my grumpiness will go off the scale, to an insane value of (<span class="math notranslate nohighlight">\(Y_i = 125.96\)</span>). Best to be avoided, I think.</p>
</div>
</div>
<div class="section" id="multiple-linear-regression">
<span id="multipleregression"></span><h2><span class="section-number">18.3. </span>Multiple linear regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code>. In fact, up to this point, <em>every</em> statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of <strong><em>multiple regression</em></strong> model would be in order?</p>
<p>Multiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> to predict the <code class="docutils literal notranslate"><span class="pre">dan_grump</span></code> variable. As before, we let <span class="math notranslate nohighlight">\(Y_i\)</span> refer to my grumpiness on the <span class="math notranslate nohighlight">\(i\)</span>-th day. But now we have two <span class="math notranslate nohighlight">\(X\)</span> variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let <span class="math notranslate nohighlight">\(X_{i1}\)</span> refer to the hours I slept on the <span class="math notranslate nohighlight">\(i\)</span>-th day, and <span class="math notranslate nohighlight">\(X_{i2}\)</span> refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:</p>
<div class="math notranslate nohighlight">
\[
Y_i = b_2 X_{i2} + b_1 X_{i1} + b_0 + \epsilon_i
\]</div>
<p>As before, <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the residual associated with the <span class="math notranslate nohighlight">\(i\)</span>-th observation, <span class="math notranslate nohighlight">\(\epsilon_i = {Y}_i - \hat{Y}_i\)</span>. In this model, we now have three coefficients that need to be estimated: <span class="math notranslate nohighlight">\(b_0\)</span> is the intercept, <span class="math notranslate nohighlight">\(b_1\)</span> is the coefficient associated with my sleep, and <span class="math notranslate nohighlight">\(b_2\)</span> is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients <span class="math notranslate nohighlight">\(\hat{b}_0\)</span>, <span class="math notranslate nohighlight">\(\hat{b}_1\)</span> and <span class="math notranslate nohighlight">\(\hat{b}_2\)</span> are those that minimise the sum squared residuals.</p>
</div>
<div class="section" id="multiple-linear-regression-in-python">
<span id="pingouinmultiplelinearregression"></span><h2><span class="section-number">18.4. </span>Multiple Linear Regression in Python<a class="headerlink" href="#multiple-linear-regression-in-python" title="Permalink to this headline">¶</a></h2>
<p>Doing mulitiple linear regression in <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> is just as easy as adding some more predictor variables, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span> <span class="n">lmm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Still, there is one thing to watch out for. If you look carefully at the command above, you will notice that not only have we added a new predictor (<code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code>), we have also added some extra brackets. While before our predictor variable was <code class="docutils literal notranslate"><span class="pre">['dan_sleep']</span></code>, now we have <code class="docutils literal notranslate"><span class="pre">[['dan_sleep',</span> <span class="pre">'baby_sleep']]</span></code>. Why the extra set of <code class="docutils literal notranslate"><span class="pre">[]</span></code>?</p>
<p>This is because we are using the brackets in two different ways. When we wrote <code class="docutils literal notranslate"><span class="pre">['dan_sleep']</span></code>, the square brackets mean “select the column with the header ‘dan_sleep’”. But now we are giving <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> a <em>list</em> of columns to select, and <code class="docutils literal notranslate"><span class="pre">list</span></code> objects are <em>also</em> defined by square brackets in Python. To keep things clear, another way to achieve the same result would be to define the list of predictor variables outside the call to <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="s1">&#39;dan_grump&#39;</span>

<span class="n">lmm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">outcome</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>You could do all the work outside of <code class="docutils literal notranslate"><span class="pre">pinguoin</span></code>, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">lmm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>All three of these will give the same result, so it’s up to you choose what makes most sense to you. But now it’s time to take a look at the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lmm</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The coefficient associated with dan_sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby_sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets; not really. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, <a class="reference internal" href="#fig-sleep-regressions-3d"><span class="std std-numref">Fig. 18.4</span></a> shows a 3D plot that plots all three variables, along with the regression model itself.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># style the plot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>

<span class="c1"># construct 3d plot space</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> 
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># define axes</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;baby_sleep&#39;</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="c1"># set axis labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;dan_sleep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;baby_sleep&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;dan_grump&quot;</span><span class="p">)</span>


<span class="c1"># get intercept and regression coefficients from the lmm model</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lmm</span><span class="p">[</span><span class="s1">&#39;coef&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">lmm</span><span class="p">[</span><span class="s1">&#39;coef&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># create a 3d plane representation of the lmm predictions</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">zs</span> <span class="o">=</span> <span class="n">xs</span><span class="o">*</span><span class="n">coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">ys</span><span class="o">*</span><span class="n">coefs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">intercept</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="n">zs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># plot the data and plane</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">ys</span><span class="p">,</span><span class="n">zs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="c1"># adjust the viewing angle</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">97</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.04-regression_34_0.png" src="_images/05.04-regression_34_0.png" />
</div>
</div>
<div class="figure align-default" id="fig-sleep-regressions-3d" style="width: 600px">
<p class="caption"><span class="caption-number">Fig. 18.4 </span><span class="caption-text">A 3D visualisation of a multiple regression model. There are two predictors in the model, <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code>; the outcome variable is <code class="docutils literal notranslate"><span class="pre">dan.grump</span></code>. Together, these three variables form a 3D space: each observation (blue dots) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients, what we’re trying to do is find a plane that is as close to all the blue dots as possible.</span><a class="headerlink" href="#fig-sleep-regressions-3d" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="formula-for-the-general-case">
<h3><span class="section-number">18.4.1. </span>Formula for the general case<a class="headerlink" href="#formula-for-the-general-case" title="Permalink to this headline">¶</a></h3>
<p>The equation that I gave above shows you what a multiple regression model looks like when you include two predictors. Not surprisingly, then, if you want more than two predictors all you have to do is add more <span class="math notranslate nohighlight">\(X\)</span> terms and more <span class="math notranslate nohighlight">\(b\)</span> coefficients. In other words, if you have <span class="math notranslate nohighlight">\(K\)</span> predictor variables in the model then the regression equation looks like this:</p>
<div class="math notranslate nohighlight">
\[
Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
\]</div>
</div>
</div>
<div class="section" id="quantifying-the-fit-of-the-regression-model">
<span id="r2"></span><h2><span class="section-number">18.5. </span>Quantifying the fit of the regression model<a class="headerlink" href="#quantifying-the-fit-of-the-regression-model" title="Permalink to this headline">¶</a></h2>
<p>So we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the <code class="docutils literal notranslate"><span class="pre">lm</span></code> model <em>claims</em> that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> about what my mood is like: my actual mood is <span class="math notranslate nohighlight">\(Y_i\)</span>. If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.</p>
<div class="section" id="the-r-2-value">
<h3><span class="section-number">18.5.1. </span>The <span class="math notranslate nohighlight">\(R^2\)</span> value<a class="headerlink" href="#the-r-2-value" title="Permalink to this headline">¶</a></h3>
<p>Once again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals:</p>
<div class="math notranslate nohighlight">
\[
\mbox{SS}_{res} = \sum_i (Y_i - \hat{Y}_i)^2
\]</div>
<p>which we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable,</p>
<div class="math notranslate nohighlight">
\[
\mbox{SS}_{tot} = \sum_i (Y_i - \bar{Y})^2
\]</div>
<p>While we’re here, let’s calculate these values in Python. Firstly, in order to make my Python commands look a bit more similar to the mathematical equations, I’ll create variables <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">]</span> <span class="c1"># the predictor</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span> <span class="c1"># the outcome</span>
</pre></div>
</div>
</div>
</div>
<p>First, lets just examine the output for the simple model that uses only a single predictor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.96</td>
      <td>3.02</td>
      <td>41.76</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.97</td>
      <td>131.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.94</td>
      <td>0.43</td>
      <td>-20.85</td>
      <td>0.0</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-9.79</td>
      <td>-8.09</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In this output, we can see that Python has calculated an intercept of 125.96 and a regression coefficient (<span class="math notranslate nohighlight">\(beta\)</span>) of -8.94. So for every hour of sleep I get, the model estimates that this will correspond to a decrease in grumpiness of about 9 on my incredibly scientific grumpiness scale. We can use this information to calculate <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, that is, the values that the model <em>predicts</em> for the outcome measure, as opposed to <span class="math notranslate nohighlight">\(Y\)</span>, which are the actual data we observed. So, for each value of the predictor variable X, we multiply that value by the regression coefficient -8.84, and add the intercept 125.97:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_pred</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.94</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="mf">125.97</span>
</pre></div>
</div>
</div>
</div>
<p>Okay, now that we’ve got a variable which stores the regression model predictions for how grumpy I will be on any given day, let’s calculate our sum of squared residuals. We would do that using the following command:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SS_resid</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">SS_resid</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1838.7224883200004
</pre></div>
</div>
</div>
</div>
<p>Wonderful. A big number that doesn’t mean very much. Still, let’s forge boldly onwards anyway, and calculate the total sum of squares as well. That’s also pretty simple:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">SS_tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">SS_tot</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9998.590000000002
</pre></div>
</div>
</div>
</div>
<p>Hm. Well, it’s a much bigger number than the last one, so this does suggest that our regression model was making good predictions. But it’s not very interpretable.</p>
<p>Perhaps we can fix this. What we’d like to do is to convert these two fairly meaningless numbers into one number. A nice, interpretable number, which for no particular reason we’ll call <span class="math notranslate nohighlight">\(R^2\)</span>. What we would like is for the value of <span class="math notranslate nohighlight">\(R^2\)</span> to be equal to 1 if the regression model makes no errors in predicting the data. In other words, if it turns out that the residual errors are zero, that is, if <span class="math notranslate nohighlight">\(\mbox{SS}_{res} = 0\)</span>, then we expect <span class="math notranslate nohighlight">\(R^2 = 1\)</span>. Similarly, if the model is completely useless, we would like <span class="math notranslate nohighlight">\(R^2\)</span> to be equal to 0. What do I mean by “useless”? Tempting as it is demand that the regression model move out of the house, cut its hair and get a real job, I’m probably going to have to pick a more practical definition: in this case, all I mean is that the residual sum of squares is no smaller than the total sum of squares, <span class="math notranslate nohighlight">\(\mbox{SS}_{res} = \mbox{SS}_{tot}\)</span>. Wait, why don’t we do exactly that? The formula that provides us with out <span class="math notranslate nohighlight">\(R^2\)</span> value is pretty simple to write down,</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\mbox{SS}_{res}}{\mbox{SS}_{tot}}
\]</div>
<p>and equally simple to calculate in Python:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span> <span class="p">(</span><span class="n">SS_resid</span> <span class="o">/</span> <span class="n">SS_tot</span><span class="p">)</span>
<span class="n">R2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.816101821524835
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(R^2\)</span> value, sometimes called the <strong><em>coefficient of determination</em></strong><a class="footnote-reference brackets" href="#notenever" id="id5">5</a> has a simple interpretation: it is the <em>proportion</em> of the variance in the outcome variable that can be accounted for by the predictor. So in this case, the fact that we have obtained <span class="math notranslate nohighlight">\(R^2 = .816\)</span> means that the predictor (<code class="docutils literal notranslate"><span class="pre">my.sleep</span></code>) explains 81.6% of the variance in the outcome (<code class="docutils literal notranslate"><span class="pre">my.grump</span></code>).</p>
<p>Naturally, you don’t actually need to type in all these commands yourself if you want to obtain the <span class="math notranslate nohighlight">\(R^2\)</span> value for your regression model. And as you have probably already noticed, <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> calculates <span class="math notranslate nohighlight">\(R^2\)</span>  for us without even being asked to. But there’s another property of <span class="math notranslate nohighlight">\(R^2\)</span> that I want to point out.</p>
</div>
<div class="section" id="the-relationship-between-regression-and-correlation">
<h3><span class="section-number">18.5.2. </span>The relationship between regression and correlation<a class="headerlink" href="#the-relationship-between-regression-and-correlation" title="Permalink to this headline">¶</a></h3>
<p>At this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol <span class="math notranslate nohighlight">\(r\)</span> to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient <span class="math notranslate nohighlight">\(r\)</span> and the <span class="math notranslate nohighlight">\(R^2\)</span> value from linear regression? Of course there is: the squared correlation <span class="math notranslate nohighlight">\(r^2\)</span> is identical to the <span class="math notranslate nohighlight">\(R^2\)</span> value for a linear regression with only a single predictor. To illustrate this, here’s the squared correlation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>  <span class="c1"># calculate the correlation</span>
<span class="n">r</span><span class="o">**</span><span class="mi">2</span>    <span class="c1"># print the squared correlation</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8161027191478786
</pre></div>
</div>
</div>
</div>
<p>Yep, same number. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.</p>
</div>
<div class="section" id="the-adjusted-r-2-value">
<h3><span class="section-number">18.5.3. </span>The adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value<a class="headerlink" href="#the-adjusted-r-2-value" title="Permalink to this headline">¶</a></h3>
<p>One final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted <span class="math notranslate nohighlight">\(R^2\)</span>”. The motivation behind calculating the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value is the observation that adding more predictors into the model will <em>always</em> cause the <span class="math notranslate nohighlight">\(R^2\)</span> value to increase (or at least not decrease). The adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value introduces a slight change to the calculation, as follows. For a regression model with <span class="math notranslate nohighlight">\(K\)</span> predictors, fit to a data set containing <span class="math notranslate nohighlight">\(N\)</span> observations, the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mbox{adj. } R^2 = 1 - \left(\frac{\mbox{SS}_{res}}{\mbox{SS}_{tot}} \times \frac{N-1}{N-K-1} \right)
\]</div>
<p>This adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value is that when you add more predictors to the model, the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> value <em>can’t</em> be interpreted in the elegant way that <span class="math notranslate nohighlight">\(R^2\)</span> can. <span class="math notranslate nohighlight">\(R^2\)</span> has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model; to my knowledge, no equivalent interpretation exists for adjusted <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
<p>An obvious question then, is whether you should report <span class="math notranslate nohighlight">\(R^2\)</span> or adjusted <span class="math notranslate nohighlight">\(R^2\)</span>. This is probably a matter of personal preference. If you care more about interpretability, then <span class="math notranslate nohighlight">\(R^2\)</span> is better. If you care more about correcting for bias, then adjusted <span class="math notranslate nohighlight">\(R^2\)</span> is probably better. Speaking just for myself, I prefer <span class="math notranslate nohighlight">\(R^2\)</span>: my feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll soon see in the section on <a class="reference internal" href="#regressiontests"><span class="std std-ref">hypothesis tests for regression models</span></a>, if you’re worried that the improvement in <span class="math notranslate nohighlight">\(R^2\)</span> that you get by adding a predictor is just due to chance and not because it’s a better model, well, we’ve got hypothesis tests for that.</p>
</div>
</div>
<div class="section" id="hypothesis-tests-for-regression-models">
<span id="regressiontests"></span><h2><span class="section-number">18.6. </span>Hypothesis tests for regression models<a class="headerlink" href="#hypothesis-tests-for-regression-models" title="Permalink to this headline">¶</a></h2>
<p>So far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model; and those in which we test whether a particular regression coefficient is significantly different from zero.</p>
<p>At this point, you’re probably groaning internally, thinking that I’m going to introduce a whole new collection of tests. You’re probably sick of hypothesis tests by now, and don’t want to learn any new ones. Me too. I’m so sick of hypothesis tests that I’m going to shamelessly reuse the <span class="math notranslate nohighlight">\(F\)</span>-test from the <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">chapter on ANOVAs</span></a> and the <span class="math notranslate nohighlight">\(t\)</span>-test from <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">the chapter on t-tests</span></a>. In fact, all I’m going to do in this section is show you how those tests are imported wholesale into the regression framework.</p>
<div class="section" id="testing-the-model-as-a-whole">
<h3><span class="section-number">18.6.1. </span>Testing the model as a whole<a class="headerlink" href="#testing-the-model-as-a-whole" title="Permalink to this headline">¶</a></h3>
<p>Okay, suppose you’ve estimated your regression model. The first hypothesis test you might want to try is one in which the null hypothesis that there is <em>no relationship</em> between the predictors and the outcome, and the alternative hypothesis is that <em>the data are distributed in exactly the way that the regression model predicts</em>. Formally, our “null model” corresponds to the fairly trivial “regression” model in which we include 0 predictors, and only include the intercept term <span class="math notranslate nohighlight">\(b_0\)</span></p>
<div class="math notranslate nohighlight">
\[
H_0: Y_i = b_0 + \epsilon_i
\]</div>
<p>If our regression model has <span class="math notranslate nohighlight">\(K\)</span> predictors, the “alternative model” is described using the usual formula for a multiple regression model:</p>
<div class="math notranslate nohighlight">
\[
H_1: Y_i = \left( \sum_{k=1}^K b_{k} X_{ik} \right) + b_0 + \epsilon_i
\]</div>
<p>How can we test these two hypotheses against each other? The trick is to understand that just like we did with ANOVA, it’s possible to divide up the total variance <span class="math notranslate nohighlight">\(\mbox{SS}_{tot}\)</span> into the sum of the residual variance <span class="math notranslate nohighlight">\(\mbox{SS}_{res}\)</span> and the regression model variance <span class="math notranslate nohighlight">\(\mbox{SS}_{mod}\)</span>. I’ll skip over the technicalities, since we covered most of them in the <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">ANOVA chapter</span></a>, and just note that:</p>
<div class="math notranslate nohighlight">
\[
\mbox{SS}_{mod} = \mbox{SS}_{tot} - \mbox{SS}_{res}
\]</div>
<p>And, just like we did with the ANOVA, we can convert the sums of squares into mean squares by dividing by the degrees of freedom.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{rcl}
\mbox{MS}_{mod} &amp;=&amp; \displaystyle\frac{\mbox{SS}_{mod} }{df_{mod}} \\ \\
\mbox{MS}_{res} &amp;=&amp; \displaystyle\frac{\mbox{SS}_{res} }{df_{res} }
\end{array}
\end{split}\]</div>
<p>So, how many degrees of freedom do we have? As you might expect, the <span class="math notranslate nohighlight">\(df\)</span> associated with the model is closely tied to the number of predictors that we’ve included. In fact, it turns out that <span class="math notranslate nohighlight">\(df_{mod} = K\)</span>. For the residuals, the total degrees of freedom is <span class="math notranslate nohighlight">\(df_{res} = N -K - 1\)</span>.</p>
<p>Now that we’ve got our mean square values, you’re probably going to be entirely unsurprised (possibly even bored) to discover that we can calculate an <span class="math notranslate nohighlight">\(F\)</span>-statistic like this:</p>
<div class="math notranslate nohighlight">
\[
F =  \frac{\mbox{MS}_{mod}}{\mbox{MS}_{res}}
\]</div>
<p>and the degrees of freedom associated with this are <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(N-K-1\)</span>. This <span class="math notranslate nohighlight">\(F\)</span> statistic has exactly the same interpretation as the one we introduced <a class="reference internal" href="05.03-anova.html#anova"><span class="std std-ref">when learning about ANOVAs</span></a>. Large <span class="math notranslate nohighlight">\(F\)</span> values indicate that the null hypothesis is performing poorly in comparison to the alternative hypothesis.</p>
<p>“Ok, this is fine”, I hear you say, “but now show me the easy way! Show me how easy it is to get an <span class="math notranslate nohighlight">\(F\)</span> statistic from <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>! <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> makes everything so much easier! Surely <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> does this for me as well?</p>
<p>Yeah. About that… actually, as of the time of writing (Tuesday the 17th of May, 2022), <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> does <em>not</em> automatically calculate the <span class="math notranslate nohighlight">\(F\)</span> statistic for the model for you. This seems like kind of a strange omission to me, since it is pretty normal to report overall <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(p\)</span> values for a model, and <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> seems to be all about making the normal things easy. So, I can only assume this will get added at some point, but for now, sadly, we are left to ourselves on this one.</p>
<p>I should mention that there are other statistics packages for Python that will do this for you. <a class="reference external" href="https://www.statsmodels.org/stable/regression.html">statsmodels</a> comes to mind, for instance. But this is opening a whole new can of worms that I’d rather avoid for now, so instead I provide you with code to calculate the <span class="math notranslate nohighlight">\(F\)</span> statistic and <span class="math notranslate nohighlight">\(p\)</span>-value for the model “manually” below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span> <span class="k">as</span> <span class="n">st</span>

<span class="c1"># your predictor and outcome variables (aka, &quot;the data&quot;)</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="c1"># model the data, and store the model information in a variable called &quot;mod&quot;</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>


<span class="c1"># call the outcome data &quot;Y&quot;, just for the sake of generalizability</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">outcome</span>

<span class="c1"># get the model residuals from the model object</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">residuals_</span>

<span class="c1"># calculate the residual, the model, and the total sums of squares</span>
<span class="n">SS_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
<span class="n">SS_tot</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">SS_mod</span> <span class="o">=</span> <span class="n">SS_tot</span> <span class="o">-</span> <span class="n">SS_res</span>

<span class="c1"># get the degrees of freedom for the model and the residuals</span>
<span class="n">df_mod</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_model_</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">df_resid_</span>

<span class="c1"># caluculate the mean squares for the model and the residuals</span>
<span class="n">MS_mod</span> <span class="o">=</span> <span class="n">SS_mod</span> <span class="o">/</span> <span class="n">df_mod</span>
<span class="n">MS_res</span> <span class="o">=</span> <span class="n">SS_res</span> <span class="o">/</span> <span class="n">df_res</span>

<span class="c1"># calculate the F-statistic</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">MS_mod</span> <span class="o">/</span> <span class="n">MS_res</span>

<span class="c1"># estimate the p-value</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">f</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">df_mod</span><span class="p">,</span> <span class="n">df_res</span><span class="p">)</span>

<span class="c1"># display the results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F=&quot;</span><span class="p">,</span><span class="n">F</span><span class="p">,</span> <span class="s2">&quot;p=&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F= 215.23828653684436 p= 2.1457300163208854e-36
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tests-for-individual-coefficients">
<h3><span class="section-number">18.6.2. </span>Tests for individual coefficients<a class="headerlink" href="#tests-for-individual-coefficients" title="Permalink to this headline">¶</a></h3>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. This is important: if your regression model doesn’t produce a significant result for the <span class="math notranslate nohighlight">\(F\)</span>-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, <em>passing</em> the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the multiple linear regression model we calculated earlier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictors</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;dan_sleep&#39;</span><span class="p">,</span> <span class="s1">&#39;baby_sleep&#39;</span><span class="p">]]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;dan_grump&#39;</span><span class="p">]</span>

<span class="n">lmm</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
<span class="n">lmm</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>I can’t help but notice that the estimated regression coefficient for the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> variable is tiny (0.01), relative to the value that we get for <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> (-8.95). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this suspicious. In fact, I’m beginning to suspect that it’s really only the amount of sleep that <em>I</em> get that matters in order to predict my grumpiness.</p>
<p>Once again, we can reuse a hypothesis test that we discussed earlier, this time the <span class="math notranslate nohighlight">\(t\)</span>-test. The test that we’re interested has a null hypothesis that the true regression coefficient is zero (<span class="math notranslate nohighlight">\(b = 0\)</span>), which is to be tested against the alternative hypothesis that it isn’t (<span class="math notranslate nohighlight">\(b \neq 0\)</span>). That is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{rl}
H_0: &amp; b = 0 \\
H_1: &amp; b \neq 0 
\end{array}
\end{split}\]</div>
<p>How can we test this? Well, if the central limit theorem is kind to us, we might be able to guess that the sampling distribution of <span class="math notranslate nohighlight">\(\hat{b}\)</span>, the estimated regression coefficient, is a normal distribution with mean centred on <span class="math notranslate nohighlight">\(b\)</span>. What that would mean is that if the null hypothesis were true, then the sampling distribution of <span class="math notranslate nohighlight">\(\hat{b}\)</span> has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, <span class="math notranslate nohighlight">\(\mbox{SE}({\hat{b}})\)</span>, then we’re in luck. That’s <em>exactly</em> the situation for which we introduced the one-sample <span class="math notranslate nohighlight">\(t\)</span> way back in <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">the chapter on t-tests</span></a>. So let’s define a <span class="math notranslate nohighlight">\(t\)</span>-statistic like this,</p>
<div class="math notranslate nohighlight">
\[
t = \frac{\hat{b}}{\mbox{SE}({\hat{b})}}
\]</div>
<p>I’ll skip over the reasons why, but our degrees of freedom in this case are <span class="math notranslate nohighlight">\(df = N- K- 1\)</span>. Irritatingly, the estimate of the standard error of the regression coefficient, <span class="math notranslate nohighlight">\(\mbox{SE}({\hat{b}})\)</span>, is not as easy to calculate as the standard error of the mean that we used for the simpler <span class="math notranslate nohighlight">\(t\)</span>-tests <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">earlier</span></a>. In fact, the formula is somewhat ugly, and not terribly helpful to look at. For our purposes it’s sufficient to point out that the standard error of the  estimated regression coefficient depends on both the predictor and outcome variables, and is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).</p>
<p>In any case, this <span class="math notranslate nohighlight">\(t\)</span>-statistic can be interpreted in the same way as the <span class="math notranslate nohighlight">\(t\)</span>-statistics that we discussed <a class="reference internal" href="05.02-ttest.html#ttest"><span class="std std-ref">earlier</span></a>. Assuming that you have a two-sided alternative (i.e., you don’t really care if <span class="math notranslate nohighlight">\(b &gt;0\)</span> or <span class="math notranslate nohighlight">\(b &lt; 0\)</span>), then it’s the extreme values of <span class="math notranslate nohighlight">\(t\)</span> (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.</p>
<p>Now we are in a position to understand all the values in the multiple regression table provided by <code class="docutils literal notranslate"><span class="pre">pingouin</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lmm</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>125.97</td>
      <td>3.04</td>
      <td>41.42</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>119.93</td>
      <td>132.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dan_sleep</td>
      <td>-8.95</td>
      <td>0.55</td>
      <td>-16.17</td>
      <td>0.00</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-10.05</td>
      <td>-7.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baby_sleep</td>
      <td>0.01</td>
      <td>0.27</td>
      <td>0.04</td>
      <td>0.97</td>
      <td>0.82</td>
      <td>0.81</td>
      <td>-0.53</td>
      <td>0.55</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of <span class="math notranslate nohighlight">\(b\)</span> (e.g., 125.96 for the intercept, -8.9 for the <code class="docutils literal notranslate"><span class="pre">dan_sleep</span></code> predictor, and -0.01 for the <code class="docutils literal notranslate"><span class="pre">baby_sleep</span></code> predictor). The second column is the standard error estimate <span class="math notranslate nohighlight">\(\hat\sigma_b\)</span>. The third column gives you the <span class="math notranslate nohighlight">\(t\)</span>-statistic, and it’s worth noticing that in this table <span class="math notranslate nohighlight">\(t= \hat{b}/\mbox{SE}({\hat{b}})\)</span> every time. The fourth column gives you the actual <span class="math notranslate nohighlight">\(p\)</span> value for each of these tests.<a class="footnote-reference brackets" href="#notecorrection" id="id6">6</a> Then next column gives the <span class="math notranslate nohighlight">\(r^2\)</span> value and the adjusted <span class="math notranslate nohighlight">\(r^2\)</span> for the model, and the last two columns give us the upper and lower <a class="reference internal" href="04.03-estimation.html#ci"><span class="std std-ref">confidence interval</span></a> bounds for each estimate.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="noteepsilon"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>The <span class="math notranslate nohighlight">\(\epsilon\)</span> symbol is the Greek letter epsilon. It’s traditional to use <span class="math notranslate nohighlight">\(\epsilon_i\)</span> or <span class="math notranslate nohighlight">\(e_i\)</span> to denote a residual.</p>
</dd>
<dt class="label" id="notekungfu"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Or at least, I’m assuming that it doesn’t help most people. But on the off chance that someone reading this is a proper kung fu master of linear algebra (and to be fair, I always have a few of these people in my intro stats class), it <em>will</em> help <em>you</em> to know that the solution to the estimation problem turns out to be <span class="math notranslate nohighlight">\(\hat{b} = (X^TX)^{-1} X^T y\)</span>, where <span class="math notranslate nohighlight">\(\hat{b}\)</span> is a vector containing the estimated regression coefficients,  <span class="math notranslate nohighlight">\(X\)</span> is the “design matrix” that contains the predictor variables (plus an additional column containing all ones; strictly <span class="math notranslate nohighlight">\(X\)</span> is a matrix of the regressors, but I haven’t discussed the distinction yet), and <span class="math notranslate nohighlight">\(y\)</span> is a vector containing the outcome variable. For everyone else, this isn’t exactly helpful, and can be downright scary. However, since quite a few things in linear regression can be written in linear algebra terms, you’ll see a bunch of footnotes like this one in this chapter. If you can follow the maths in them, great. If not, ignore it.</p>
</dd>
<dt class="label" id="notenolan"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Christopher Nolan, have your people call my people if you’re interested, we’ll do lunch!</p>
</dd>
<dt class="label" id="noteformula"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>This is extra confusing if you happen to have come from the world of R, where this sort of model is usually defined with a formula, in which the outcome measure comes first, followed by the predictor(s), or even if you have used <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, which also preserves the R-style formula notation.</p>
</dd>
<dt class="label" id="notenever"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>And by “sometimes” I mean “almost never”. In practice everyone just calls it “<span class="math notranslate nohighlight">\(R\)</span>-squared”.</p>
</dd>
<dt class="label" id="notecorrection"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Note that, although <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> has done multiple tests here, it hasn’t done a Bonferroni correction or anything. These are standard one-sample <span class="math notranslate nohighlight">\(t\)</span>-tests with a two-sided alternative. If you want to make corrections for multiple tests, you need to do that yourself.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="05.03-anova.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">16. </span>Comparing several means</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="05.05-anova2.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Factorial ANOVA</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Danielle Navarro and Ethan Weed<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>